
\chapter{CSI}
The question now is, is it possible to increase the Throughput of this channel with knowledge of the channel Matrix $H$. and to investigate this we use following Channel Representation and we are fully aware of its implications and imperfections.
\\// description of our channel


 \section{Multiuser Case}
Since Water Filling diagonalizese the channel it does not matter what kind of decoder is used on the receiver side, the streams are already independent. But Waterfilling is only feasible in a single user case with full CSIT due to the fact that inter antenna data coordination is needed. So in a multi user scenario we need other methods to decode the channel.
We use two well-known Receiver structures:
\begin{enumerate}
	\item Linear MMSE Receiver\\
	//formula
	\item V-BLAST MMSE or SIC with MMSE (MMSE-SIC)\\
	//formula (algorithm)\\
	Generally speaking the SIC receivers provide better Throughput but are more complicated to implement, and harder to adapt.
\end{enumerate}

\section{Optimizing the Throughput of V-Blast}
[Jindal]
The Paper proposes an algorithm which optimizes the throughput with an SIC receiver, the papers goal is to solve the problem of beam forming in a Broadcast or Downlink Channel, so exactly the opposite direction than we are discussing in this report. But as //paper has described there is a duality of these two channels and we are able to use the optimized results of the MAC to optimize the BC. So his work is using this duality to solve the easer problem of the MAC and then transform it back to the BC problem, so his work is highly relevant to us.\\
// Duality description\\
// abstract of Jindal\\
// Jindal algorithm

\section{Comparison}
Numerical Gradient Search function
We implemented a numerical gradient search for the SIC receiver to compare to the Jindal algorithm in terms of throughput maximization. We do not compare it in terms of complexitiy or speed of convergence.
Convergence is guaranteed for the numerical Gradient based on the work of //paper. And the Jindal himself adds the proof of convergence to in his paper.
\section{Linear MMSE}
From there we started to investigate the LMMSE receiver.\\
//theory\\
// formulas\\
\section{Optimizing}
// problems //drawbacks\\
Fodor describes an algorithm in his //paper to optimize the throughput of a typical Cellular Network Channel, but to improve stability and feasibility of his algorithm he sacrifices accuracy and chooses to use a very week constraint. Which leads to unusable results in comparison to an easy numerical Gradient search of the same Optimum.

\section{Numerical Gradient}
So we will only look at the numerical Gradient for Optimizing the LMMSE receiver. As shown in // Figure there is some pretty big improvement over just equal power distribution, but even for high SNR cases at least one channel is closed, which means that at least one user is not allowed to send any data.


\chapter{Gradient search}
\section{Steepest descent}
The idea of the steepest descent algorithm is to find the global maximum or minimum of a function by searching along the gradient of the function.
\begin{algorithm}
	1.	Start at one point
	2.	Calculate the Gradient
	3.	Use the gradient to get to the next point (Step)
	4.	Iterate
\end{algorithm}
We used two ways to calculate the gradient at a certain point, numerical and analytical:

\section{Numerical Gradient}
The numerical Algorithm calculates the Gradient with numerical means. Which means to increase the input in each dimension one after another by a very small amount and recalculate output. With both results, the original one and the new ones, we can approximate the local gradient very precisely.
\begin{algorithm}
	1.	Calculate Starting Point
	2.	1:K 
	a.	Increase input K
	b.	Calculate Point
	3.	Calculate Gradient
	4.	Step to next point
	5.	Iterate
\end{algorithm}
Analytical Gradient
The analytical Algorithm on the other hand uses analytical tools to calculate the gradient form the known function in advance. It uses the gradient function to calculate the exact gradient at each point.
\begin{algorithm}
	Pre: calculate analytical Gradient by hand
	1.	Calculate gradient at input Point with the precalculated gradient function
	2.	Step to the next point
	3.	Iterate
\end{algorithm}

\section{Analytical Gradient for Throughput Maximization}
// calculations: formulas step by step\\
Implementation with the norm and not just the sum allows us to use the same function in two different ways. Firstly with a //p value of 1, we just calculate the sum Rate for the throughput maximization or sum Rate maximization. Secondly we can set the //p value to something very small like -30 to create a maxmin optimizer. The Optimum would be the //min norm but this is not feasible in the numerical domain and is prone to get stuck in local minimas, here -30 is a good approximation, we tested the algorithm with -100 and achieved results which were closer to the optimum that each channel gets the same Rate but it’s not save to use.

\section{Adaptive step Size}
One Problem that occurred during the simulations was that at some point the gradient search got unstable, in our case it started to fluctuate and stopped moving towards the optimum. The reason of this bad and unwanted behavior lies in the step size of the function, and so there are two ways to fix it. The first and easy way is just to reduce the step size and increase the number of iterations accordingly, this way the problem occurs later and weaker and has less impact on the end result. The second much more complicated solution is to introduce adaptive step size.\\
Adaptive Step size means, that for each iteration we search the optimal step size which allows us to take the biggest step towards our goal. So it should not be possible to choose a step size which gives us a worse result than we had on the iteration before.
\begin{algorithm}
	Start at an arbitrary step size, calculate the next point
	If next Point is better than last, increase step size
	If next Point is worse than last, decrease step size
	Stop if increasing or decreasing does not improve the result anymore
\end{algorithm}
With this Algorithm we can prevent any fluctuation in the gradient and force it to go the fastest possible way. We do not have to calculate the gradient as often and can thus reduce the needed calculation time. Another nice side effect is, that we implicitly know when we reached the maximum or minimum when the step size is decreased to zero without finding any better points than the previous.

\section{Implementation}
//lots of code\\

\section{Gradient for power Minimization}
// calculations\\
// differences to the throughput max\\
// question about convecity?\\

\section{Fodor Paper}

\subsection{The Algorithm}
\begin{enumerate}
	\item calculate the effective interference for MMSE processing:
		\begin{multline}
			\zeta_{k,s} = \biggl\{\biggl(d^{-\roh}_{k,k}\chi_{k,k}H^H_{k,k}\biggl(\sum_{j\neqk}{P_jd^{-\roh}_{k,j}\chi_{k,j}H_{k,j}T_jT_j^HH^H_{k,j}}\\
			+N_t\sigma^2_nI\biggr)^{-1}H_{k,k}+\frac{1}{P_k}I\biggr)^{-1}\biggr\}^{(s,s)}
		\end{multline}

	\item calculate the optimal loading matrix
		\begin{equation}
			(T_k)^{(s,s)} = \sqrt{\frac{\zeta_{k,s}N_t}{\sum_{j=1}^{N_t}\zeta_{k,j}}}\;\forall s\in[1,N_t]
		\end{equation}

	\item calculate used Power
		\begin{equation}
			P_k = \frac{\zeta_{k,s}}{\vert(T_k)^{(s,s)}\vert^2}(\gamma_{tgt}+1)\;\forall k
		\end{equation}

	\item[n.] until no more change

\end{enumerate}

\subsection{closed form Solution}
Fodor reuses a poroposition for a closed form solution from xy.
$p^*$ is calculated from ... with ---